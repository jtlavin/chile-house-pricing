# ğŸ  Chile House Pricing - MLOps Project

An end-to-end MLOps project for predicting house prices in Santiago, Chile. This project demonstrates the complete machine learning lifecycle from data collection to model deployment.

## ğŸ“‹ Project Overview

This project is structured as a **monorepo** containing:
- **Data Pipeline**: Web scraping from Portal Inmobiliario
- **ML Development**: Feature engineering, model training, and experimentation
- **API**: FastAPI service for serving predictions
- **Infrastructure**: Docker setup and deployment scripts

**Goal**: Learn and implement MLOps best practices while building a real-world house price prediction system.

## ğŸ—ï¸ Project Structure

```
chile-house-pricing/
â”‚
â”œâ”€â”€ data-pipeline/              # Data Collection & ETL
â”‚   â”œâ”€â”€ scrapers/              # Web scraping modules
â”‚   â”‚   â”œâ”€â”€ models.py          # PropertyData and ScrapingConfig
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py    # Respectful rate limiting
â”‚   â”‚   â””â”€â”€ portal_inmobiliario.py  # Main scraper
â”‚   â”œâ”€â”€ database/              # Database management
â”‚   â”‚   â””â”€â”€ db_manager.py      # SQLite operations
â”‚   â””â”€â”€ jobs/                  # Scheduled jobs
â”‚       â””â”€â”€ scrape_job.py      # Main scraping job
â”‚
â”œâ”€â”€ ml/                        # Machine Learning
â”‚   â”œâ”€â”€ data/                  # Datasets
â”‚   â”‚   â”œâ”€â”€ raw/              # Scraped data
â”‚   â”‚   â”œâ”€â”€ processed/        # Cleaned data
â”‚   â”‚   â””â”€â”€ splits/           # Train/val/test
â”‚   â”œâ”€â”€ notebooks/            # Jupyter notebooks for EDA
â”‚   â”œâ”€â”€ src/                  # ML source code
â”‚   â”‚   â”œâ”€â”€ features/         # Feature engineering
â”‚   â”‚   â”œâ”€â”€ models/           # Training & evaluation
â”‚   â”‚   â””â”€â”€ utils/            # Utilities
â”‚   â”œâ”€â”€ experiments/          # MLflow tracking
â”‚   â””â”€â”€ models/               # Saved models
â”‚
â”œâ”€â”€ api/                       # Model Serving API
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI app
â”‚   â”‚   â”œâ”€â”€ routers/          # API endpoints
â”‚   â”‚   â”œâ”€â”€ schemas/          # Pydantic models
â”‚   â”‚   â””â”€â”€ services/         # Business logic
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ infrastructure/            # DevOps & MLOps
â”‚   â”œâ”€â”€ docker/               # Docker configurations
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”‚   â”œâ”€â”€ Dockerfile.scraper
â”‚   â”‚   â””â”€â”€ Dockerfile.api
â”‚   â””â”€â”€ scripts/              # Helper scripts
â”‚       â”œâ”€â”€ setup.sh
â”‚       â””â”€â”€ run_scraper.sh
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ data-pipeline-guide.md
â”‚   â”œâ”€â”€ scraper-usage.md
â”‚   â””â”€â”€ web-scraping-study-guide.md
â”‚
â””â”€â”€ pyproject.toml            # Project dependencies

```

## ğŸš€ Quick Start

### 1. Setup

Run the setup script:
```bash
bash infrastructure/scripts/setup.sh
```

Or manually:
```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e .

# Install Playwright browsers
playwright install chromium
```

### 2. Run the Data Pipeline

```bash
# Run demo (scrapes 3 properties)
python -m data-pipeline.jobs.scrape_job --demo

# Or use the script
bash infrastructure/scripts/run_scraper.sh --demo

# Interactive mode (choose scraping options)
python -m data-pipeline.jobs.scrape_job
```

### 3. Start the API

```bash
uvicorn api.app.main:app --reload
```

Visit http://localhost:8000/docs for interactive API documentation.

### 4. Explore the Data (Coming Soon)

```bash
# Install ML dependencies
pip install -e ".[ml,viz]"

# Start Jupyter
jupyter notebook ml/notebooks
```

## ğŸ“ Learning Path

This project is designed as a learning journey through MLOps:

### Phase 1: Data Pipeline âœ… (Current)
- [x] Web scraping with Playwright
- [x] Data storage (SQLite)
- [x] Modular code structure
- [ ] Scheduled data collection
- [ ] Data validation with Great Expectations

### Phase 2: ML Development (Next)
- [ ] Exploratory Data Analysis (EDA)
- [ ] Feature engineering
- [ ] Model training (XGBoost, LightGBM)
- [ ] Experiment tracking with MLflow
- [ ] Hyperparameter tuning with Optuna
- [ ] Model evaluation & validation

### Phase 3: Model Serving
- [ ] Complete API implementation
- [ ] Model loading & inference
- [ ] API testing
- [ ] Input validation

### Phase 4: MLOps
- [ ] Docker containerization
- [ ] CI/CD with GitHub Actions
- [ ] Model monitoring
- [ ] A/B testing infrastructure
- [ ] Prometheus + Grafana dashboards

## ğŸ“Š Current Data

The project has scraped initial property data:
- **Location**: Portal Inmobiliario listings
- **Area**: Las Condes, Santiago
- **Format**: SQLite database + JSON files
- **Path**: `ml/data/raw/`

## ğŸ› ï¸ Tech Stack

### Data Collection
- **Playwright**: Web scraping
- **SQLite**: Local database

### ML & Data Science
- Pandas, NumPy (data manipulation)
- Scikit-learn (preprocessing, baseline models)
- XGBoost, LightGBM (gradient boosting)
- MLflow (experiment tracking)
- Optuna (hyperparameter tuning)

### API & Deployment
- **FastAPI**: REST API
- **Pydantic**: Data validation
- **Docker**: Containerization
- **Uvicorn**: ASGI server

## ğŸ“– Documentation

Detailed documentation is available in the `docs/` folder:
- [Data Pipeline Guide](docs/data-pipeline-guide.md) - Technical details of the scraper
- [Scraper Usage](docs/scraper-usage.md) - How to use the scraper
- [Web Scraping Study Guide](docs/web-scraping-study-guide.md) - Learn Playwright basics

## ğŸ§ª Testing

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests (when implemented)
pytest
```

## ğŸ³ Docker Usage

```bash
# Build and run all services
docker-compose -f infrastructure/docker/docker-compose.yml up

# Run only the API
docker-compose -f infrastructure/docker/docker-compose.yml up api

# Run scraper
docker-compose -f infrastructure/docker/docker-compose.yml run scraper
```

## ğŸ“ Development Workflow

1. **Scrape Data**: Collect property listings
2. **Explore Data**: Jupyter notebooks for EDA
3. **Engineer Features**: Create meaningful features
4. **Train Models**: Experiment with different algorithms
5. **Track Experiments**: Use MLflow
6. **Deploy Model**: Update API with best model
7. **Monitor**: Track performance in production

## ğŸ¤ Contributing

This is a learning project, but suggestions and improvements are welcome!

## ğŸ“œ License

MIT License

## ğŸ™ Acknowledgments

- Portal Inmobiliario for property data
- Built following MLOps best practices

---

**Status**: ğŸš§ Active Development - Phase 1 (Data Pipeline) Complete
